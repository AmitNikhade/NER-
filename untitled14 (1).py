# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LASw4dfMBa9F2d5GduRoMxvKqYAY66Nl

## Named Entity Recognition using Bidirectional Encoder Representations from Transformers(BERT)

Install simpletransformers library, you can use huggingface too as per your confortability. I tried running this code on kaggle notebook but during compiling the cells it throwed lot's of error related to the tokenizer and transformers packages, so be sure to use google colab to run the code for NER.

Bert is just a transformer based model which is the encoder part of the transformer, In such a way that the encoders are stacked one below another. The encoder consists of a starting input embedding layer >> Positional embedding >> Multihead-Attention >> Layer_Normalization >> Feed_forward_Network >> layer_normalization. 

And such mechanism is repeated which is called bert, that can access sequences from both sides of the encoder i.e bidirectional encoder.
"""

# Installing simpletransformers libraries
pip install simpletransformers

"""Import Libraries"""

import pandas as pd
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from simpletransformers.ner import NERModel,NERArgs
from sklearn.metrics import accuracy_score
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
torch.__version__

import numpy as np
from tqdm import tqdm, trange
import logging

"""Extract the zip file"""

from zipfile import ZipFile
f = ZipFile('/content/drive/MyDrive/archive.zip')
f.extractall()

"""Load Data"""

data = pd.read_csv("/content/ner_dataset.csv", encoding="latin1").fillna(method="ffill")
data.tail(10)

# No null values
data.isnull().any()

"""Pre-Processing Text"""

data["Sentence #"] = LabelEncoder().fit_transform(data["Sentence #"] )
data.rename(columns={"Sentence #":"sentence_id","Word":"words","Tag":"labels"}, inplace =True)

X= data[["sentence_id","words"]]
Y =data["labels"]

x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size =0.2)

train_data = pd.DataFrame({"sentence_id":x_train["sentence_id"],"words":x_train["words"],"labels":y_train})
test_data = pd.DataFrame({"sentence_id":x_test["sentence_id"],"words":x_test["words"],"labels":y_test})

label = data["labels"].unique().tolist()

"""FineTuning Bert"""

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

train_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "use_early_stopping":True,
    "weight_decay": 0.01,
    "do_lower_case":False,
    "num_train_epochs": 1,
    "learning_rate": 1e-4,
    "overwrite_output_dir":True,
    "train_batch_size": 32,
    "eval_batch_size": 32
}

model = NERModel('bert', 'bert-base-cased',labels=label,args =train_args)

"""Training time"""

model.train_model(train_data,eval_data = test_data,acc=accuracy_score)

Model evaluation

results, model_outputs, predictions = model.eval_model(test_data)

"""Performance """

results

model_outputs

"""Predictions over the test data"""

predictions

prediction, model_output = model.predict(["I think you live in Sydney, Right?"])

"""Prediction over a sample text"""

prediction

